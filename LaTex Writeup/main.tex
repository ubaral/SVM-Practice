\documentclass[11pt, fleqn]{article}
\setlength{\parindent}{20pt}
\usepackage[fleqn]{amsmath}
\usepackage{textcomp,amssymb,enumitem,amsthm,subfig,pgf,listings,hyperref, etoolbox,graphicx}
\usepackage[T1]{fontenc}

\def\Name{Utsav Baral}  %Name
\def\SID{25694452}  %Student ID number
\def\Homework{1} % Number of Homework
\def\Session{Spring 2016}

\title{CS 189 --Spring 2016 --- Homework \Homework\ Solutions}
\author{\large{\Name, SID \SID}}
\date{DUE: February 10th, 2016}
\markboth{CS 189--\Session\ Homework \Homework\ \Name}{CS 189--\Session\ Homework \Homework\ \Name}
\pagestyle{myheadings}

\textheight=9in
\textwidth=6.5in
\topmargin=-.75in
\oddsidemargin=0.25in
\evensidemargin=0.25in

\newcommand{\PartialD}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\RegularD}[2]{\frac{d #1}{d #2}}
\newcommand{\vectSpace}[0]{\mathbf{V}}
\renewcommand\qedsymbol{$\blacksquare$}

\makeatletter
\patchcmd{\l@section}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\newcommand{\mysection}[2]{
    \setcounter{section}{#1}
    \section*{#2}
    \addcontentsline{toc}{section}{#2}
}
\renewcommand{\subsection}[2]{
    \setcounter{subsection}{#1}
    \section*{#2}
    \addcontentsline{toc}{subsection}{#2}
}

\usepackage{inconsolata}
\lstset{basicstyle=\scriptsize\ttfamily,breaklines=true,showtabs=true, showspaces=false, showstringspaces=false, breakatwhitespace=true, tabsize=1, resetmargins=true, xleftmargin=0pt, frame=none} 

\lstset{framextopmargin=50pt,frame=bottomline}

\begin{document}
\maketitle
\vspace{2ex}
\tableofcontents

\newpage
\mysection{1}{Problem 1.}
    \textit{Train a linear SVM using raw pixels as features. Plot the error rate on a validation set versus thenumber of training examples that you used to train your classifier.}\\\par
    Here is the plot I generated by using a linear kernal svm from the scikit-learn library. The Number of Training samples is plotted against the Accuracy Rate. The number of samples, per trial are $[100, 200, 500, 1000, 2000, 5000, 10000]$, respectively. And the corresponding Accuracy Rates are $[0.6961, 0.7463, 0.8054, 0.8398, 0.8576, 0.8618, 0.8688]$:\\
    \scalebox{.8}{\input{graphics/figure_1.pgf}}\\

\newpage
\mysection{2}{Problem 2.}
	\textit{Create confusion matrices for each experiment in Problem 1. Color code and report your results. You may use built-in implementations to generate confusion matrices. What insights can you get about the performance of your algorithm from looking at the confusion matrix?}\\\par
	The confusion Matrix gives us an easily visualizable way to look at the performance of the classifier.

	Along the diagonals we will see how many times the predicted value matched the actual value. The row number corresponds to what the actual digit was and the column number represents what digit it was predicted to be. We can also see which wrong thing it was classified most as. Since we are working with raw pixels as the features, it will probably missclassify similar looking shapes. Which is confirmed in the matrix by numbers like 4, which gets heavily misclassified as a 9; or 5 which often gets missclassified as 3, and 8 also gets frequently misclassified as a 3.

	Additionally, looking at the same element but in the Transposed version of the Matrix, if two digits, say $a$ and $b$, are similar we can see how many of $a$ gets missclassified as $b$, or how many images of $b$ get misclassified as $a$. Which would be usefull in fine tuning what features we may or may not want.

	Here are the actual Confusion Matrices from the different trials, with the actual values as well as color coded for easier visualization:\\[5ex]\underline{\textbf{\large{100 Training Samples:}}}\\
		$\begin{bmatrix}
					825 & 0 & 19 & 0 & 6 & 18 & 23 & 4 & 57 & 4\\
					0 & 1011 & 21 & 33 & 9 & 1 & 2 & 15 & 46 & 10\\
					28 & 42 & 633 & 101 & 48 & 6 & 42 & 25 & 14 & 2\\
					36 & 40 & 51 & 744 & 6 & 33 & 38 & 18 & 9 & 81\\
					7 & 31 & 14 & 4 & 643 & 0 & 17 & 34 & 3 & 238\\
					35 & 41 & 18 & 127 & 23 & 419 & 89 & 9 & 10 & 135\\
					28 & 52 & 37 & 2 & 42 & 21 & 820 & 0 & 11 & 0\\
					17 & 42 & 5 & 35 & 22 & 0 & 1 & 732 & 0 & 202\\
					5 & 112 & 50 & 284 & 17 & 33 & 31 & 53 & 289 & 87\\
					9 & 22 & 21 & 33 & 83 & 1 & 4 & 69 & 5 & 725
				\end{bmatrix}
				\raisebox{-18ex}{\scalebox{.4}{\input{graphics/confusionMat_100.pgf}}}\\
				$\newpage\noindent$
				\underline{\textbf{\large{200 Training Samples:}}}\\
				\begin{bmatrix}
					923 & 0 & 5 & 20 & 2 & 8 & 31 & 4 & 4 & 8\\
					1 & 1042 & 3 & 21 & 0 & 4 & 1 & 4 & 12 & 7\\
					53 & 74 & 688 & 56 & 44 & 4 & 51 & 14 & 24 & 13\\
					19 & 34 & 31 & 851 & 2 & 62 & 6 & 11 & 20 & 13\\
					5 & 31 & 1 & 1 & 673 & 2 & 5 & 9 & 7 & 198\\
					35 & 78 & 7 & 161 & 39 & 456 & 15 & 8 & 72 & 36\\
					32 & 33 & 14 & 9 & 45 & 11 & 800 & 0 & 9 & 2\\
					30 & 93 & 16 & 16 & 43 & 3 & 1 & 820 & 3 & 36\\
					13 & 91 & 50 & 165 & 25 & 34 & 27 & 19 & 563 & 30\\
					10 & 39 & 1 & 26 & 105 & 5 & 0 & 73 & 17 & 682\\
				\end{bmatrix}
				\raisebox{-18ex}{\scalebox{.4}{\input{graphics/confusionMat_200.pgf}}}\\
				\underline{\textbf{\large{500 Training Samples:}}}\\
				\begin{bmatrix}
					870 & 1 & 9 & 4 & 3 & 27 & 16 & 12 & 13 & 1\\
					0 & 1068 & 10 & 8 & 1 & 4 & 1 & 0 & 12 & 6\\
					12 & 34 & 740 & 20 & 19 & 6 & 26 & 51 & 11 & 7\\
					6 & 32 & 44 & 778 & 2 & 66 & 7 & 20 & 55 & 38\\
					6 & 7 & 4 & 3 & 827 & 6 & 24 & 5 & 4 & 61\\
					9 & 42 & 9 & 80 & 13 & 651 & 15 & 7 & 19 & 22\\
					17 & 20 & 18 & 0 & 18 & 45 & 882 & 0 & 9 & 0\\
					13 & 49 & 12 & 4 & 30 & 7 & 3 & 904 & 4 & 68\\
					7 & 52 & 41 & 39 & 15 & 68 & 15 & 27 & 676 & 67\\
					16 & 20 & 13 & 12 & 80 & 5 & 1 & 71 & 4 & 814\\
				\end{bmatrix}
				\raisebox{-18ex}{\scalebox{.4}{\input{graphics/confusionMat_500.pgf}}}\\
				\underline{\textbf{\large{1000 Training Samples:}}}\\
				\begin{bmatrix}
					916 & 1 & 10 & 5 & 1 & 21 & 11 & 7 & 7 & 10\\
					0 & 1143 & 2 & 1 & 1 & 4 & 1 & 5 & 6 & 5\\
					23 & 39 & 785 & 13 & 15 & 12 & 28 & 10 & 25 & 6\\
					28 & 17 & 46 & 834 & 1 & 56 & 14 & 21 & 29 & 23\\
					7 & 12 & 8 & 1 & 892 & 6 & 28 & 7 & 2 & 55\\
					23 & 17 & 16 & 85 & 10 & 670 & 20 & 6 & 16 & 21\\
					26 & 14 & 37 & 2 & 14 & 23 & 829 & 1 & 2 & 0\\
					2 & 25 & 34 & 5 & 18 & 10 & 0 & 856 & 2 & 85\\
					15 & 53 & 20 & 59 & 16 & 74 & 17 & 10 & 643 & 29\\
					17 & 10 & 12 & 14 & 103 & 14 & 2 & 59 & 7 & 757\\
				\end{bmatrix}
				\raisebox{-18ex}{\scalebox{.4}{\input{graphics/confusionMat_1000.pgf}}}\\
				$\newpage\noindent$
				\underline{\textbf{\large{2000 Training Samples:}}}\\
				\begin{bmatrix}
					947 & 0 & 6 & 5 & 6 & 19 & 9 & 0 & 12 & 1\\
					0 & 1075 & 8 & 0 & 1 & 5 & 0 & 5 & 8 & 1\\
					19 & 27 & 818 & 18 & 30 & 16 & 34 & 13 & 25 & 7\\
					17 & 23 & 22 & 880 & 3 & 54 & 0 & 12 & 27 & 8\\
					7 & 9 & 7 & 1 & 861 & 11 & 8 & 5 & 2 & 52\\
					28 & 13 & 9 & 74 & 14 & 696 & 13 & 4 & 31 & 16\\
					13 & 5 & 16 & 1 & 29 & 32 & 834 & 0 & 10 & 0\\
					9 & 15 & 21 & 12 & 18 & 7 & 0 & 890 & 6 & 60\\
					7 & 52 & 21 & 53 & 14 & 47 & 14 & 3 & 719 & 21\\
					19 & 13 & 6 & 24 & 75 & 6 & 1 & 72 & 13 & 820\\
				\end{bmatrix}
				\raisebox{-18ex}{\scalebox{.4}{\input{graphics/confusionMat_2000.pgf}}}\\
				\underline{\textbf{\large{5000 Training Samples:}}}\\
				\begin{bmatrix}
					921 & 0 & 4 & 2 & 4 & 12 & 12 & 1 & 10 & 2\\
					2 & 1102 & 11 & 8 & 2 & 4 & 2 & 1 & 15 & 0\\
					17 & 18 & 830 & 26 & 17 & 5 & 29 & 23 & 28 & 3\\
					16 & 18 & 34 & 841 & 2 & 49 & 3 & 14 & 54 & 14\\
					7 & 6 & 11 & 0 & 851 & 1 & 11 & 11 & 3 & 46\\
					20 & 20 & 17 & 58 & 22 & 719 & 26 & 4 & 20 & 11\\
					13 & 9 & 24 & 2 & 19 & 10 & 885 & 0 & 1 & 0\\
					7 & 13 & 18 & 12 & 14 & 1 & 0 & 898 & 5 & 56\\
					9 & 38 & 35 & 71 & 12 & 45 & 12 & 15 & 718 & 17\\
					11 & 17 & 5 & 18 & 62 & 7 & 0 & 53 & 10 & 838\\
				\end{bmatrix}
				\raisebox{-18ex}{\scalebox{.4}{\input{graphics/confusionMat_5000.pgf}}}\\
				\underline{\textbf{\large{10000 Training Samples:}}}\\
				\begin{bmatrix}
					924 & 0 & 10 & 4 & 6 & 24 & 3 & 1 & 4 & 1\\
					2 & 1086 & 3 & 1 & 2 & 3 & 2 & 1 & 8 & 2\\
					17 & 20 & 874 & 28 & 18 & 5 & 20 & 17 & 22 & 1\\
					15 & 13 & 37 & 849 & 4 & 40 & 5 & 24 & 20 & 8\\
					4 & 6 & 19 & 3 & 885 & 1 & 11 & 7 & 2 & 48\\
					23 & 13 & 8 & 87 & 20 & 684 & 9 & 6 & 38 & 2\\
					17 & 4 & 24 & 1 & 12 & 17 & 872 & 2 & 1 & 0\\
					2 & 9 & 27 & 9 & 27 & 2 & 2 & 879 & 2 & 83\\
					15 & 40 & 36 & 50 & 8 & 45 & 14 & 17 & 746 & 14\\
					5 & 4 & 14 & 9 & 84 & 8 & 0 & 58 & 12 & 82\\
				\end{bmatrix}\raisebox{-18ex}{\scalebox{.4}{\input{graphics/confusionMat_10000.pgf}}}$

\newpage
\mysection{3}{Problem 3.}\textit{Explain why cross-validation helps. Implement cross-validation and find the optimal value of the parameter C using 10-fold cross-validation on the training set with 10,000 examples. Train a linear SVM with this value of C. Please report your C value, the validation error rate, and your Kaggle score. If you used additional features, please (briefly) describe what features you added, removed, or modified.}\\[1ex]\par
Cross Validation is a powerful way to help prevent overfitting and make sure that the fit we are making will be good on real world practical applications. Additionally, at the tradeoff of time, we are getting use all the labeled data we have for validation purposes, by iterating through the k partitions and using the left over for training each time. Taking the average then gives us reliable information over the repeated testing, and on different data each time. It's a win win, except for the time consumption! Here is the output when printing out accuracy results for different c values in the code:\\[2ex]
\noindent
average accuracy for C = 1000.0 was 0.8689\\
average accuracy for C = 100.0 was 0.8689\\
average accuracy for C = 1.0was 0.8692\\
average accuracy for C = 0.1 was 0.8689\\
average accuracy for C = 0.01 was 0.8689\\
average accuracy for C = 0.001 was 0.8689\\
average accuracy for C = 0.0001 was 0.8693\\
average accuracy for C = 1e-05 was 0.8802\\
average accuracy for C = 1e-06 was 0.9018 <---Best from Trials.\\
average accuracy for C = 1e-08 was 0.8454\\
average accuracy for C = 1e-10 was 0.10869999999999999 <---lolol too soft!\\

\noindent
Zooming in the 1e-6 range:\\
average accuracy for C = 2e-06 was 0.9025000000000001\\
average accuracy for C = 1e-06 was 0.9059000000000001 <---Best Again.\\
average accuracy for C = 5e-07 was 0.9047000000000001\\[5ex]
\textbf{*Kaggle Score: 0.92960*}\\[1ex]
\textbf{Additional Features :} I added a feature to detect the amount of loops that a given sample image has. For example a 9 typically has one loop, and a 8 typically has two loops. I added a feature for the amount of surface area and also dividing the images into quadrants.

\newpage
\mysection{4}{Problem 4.}\textit{Use your cross-validation implementation from above to train a linear SVM for your spam dataset. Please report your C value, the validation error rate, and your Kaggle score. If you mod- ified the spam features, please (briefly) describe what features you added, removed, or modified.}\\[1ex]\par
\noindent
Here are the C values, didn't add any more features than given, since I'm running out of time. I simply fixed up my cross-validation implementation to work on the spam data. It is clear from the data that we do not a soft classifier but rather something harder with higher c values.\\[2ex]
average accuracy for C = 10 was 0.8022041763341067\\
average accuracy for C = 1 was 0.8020108275328691\\
average accuracy for C = 0.1 was 0.7964037122969838\\
average accuracy for C = 1e-06 was 0.7099767981438515\\

\textbf{*Kaggle Score: 0.73753*}\\[1ex]




\newpage
\mysection{6}{APPENDIX A: CODE LISTINGS}
- - - - - - - - -
\vspace{-5ex}
\subsection{1}{linearKernel\_svm\_plot.py}
\vspace{-3ex}
- - - - - - - - -
\lstinputlisting[language=Python]{../linearKernel_svm_plot.py}

\newpage\noindent
- - - - - - - - - - - - - - - - - - - - - - - - - -
\vspace{-5ex}
\subsection{2}{plot\_confusion\_matrix.py}
\vspace{-3ex}
- - - - - - - - - - - - - - - - - - - - - - - - - -
\lstinputlisting[language=Python]{../plot_confusion_matrix.py}

\newpage\noindent
- - - - - - - - - - - - - - - - - - - - - - - - - -
\vspace{-5ex}
\subsection{3}{ten\_fold\_crossValidation.py}
\vspace{-3ex}
- - - - - - - - - - - - - - - - - - - - - - - - - -
\lstinputlisting[language=Python]{../ten_fold_crossValidation.py}

\newpage\noindent
- - - - - - - - - - - - - - - - - - - - - - - - - -
\vspace{-5ex}
\subsection{3}{classifierWithExtraFeatures.py}
\vspace{-3ex}
- - - - - - - - - - - - - - - - - - - - - - - - - -
\lstinputlisting[language=Python]{../classifierWithExtraFeatures.py}

\newpage\noindent
- - - - - - - - - - - - - - - - - - - - - - - - - -
\vspace{-5ex}
\subsection{3}{SpamDataset.py}
\vspace{-3ex}
- - - - - - - - - - - - - - - - - - - - - - - - - -
\lstinputlisting[language=Python]{../SpamDataset.py}

\end{document}